======================================================================
INICIALIZACIÓN DEL SISTEMA
======================================================================
[OK] Carpetas creadas exitosamente
[INFO] Datos: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados
[INFO] Figuras: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\figures
[INFO] Resultados: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\results

======================================================================
PASO 1: CARGANDO LOS DATOS
======================================================================
[INFO] Buscando dataset en: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\student_lifestyle_dataset.csv
[OK] Dataset cargado con éxito
[INFO] Forma del dataset: (2000, 8)

[INFO] Primeras filas:
   Student_ID  Study_Hours_Per_Day  ...   GPA  Stress_Level
0           1                  6.9  ...  2.99      Moderate
1           2                  5.3  ...  2.75           Low
2           3                  5.1  ...  2.67           Low
3           4                  6.5  ...  2.88      Moderate
4           5                  8.1  ...  3.51          High

[5 rows x 8 columns]

======================================================================
EXPLORACIÓN DEL DATASET
======================================================================

[INFO] Información general del dataset:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2000 entries, 0 to 1999
Data columns (total 8 columns):
 #   Column                           Non-Null Count  Dtype  
---  ------                           --------------  -----  
 0   Student_ID                       2000 non-null   int64  
 1   Study_Hours_Per_Day              2000 non-null   float64
 2   Extracurricular_Hours_Per_Day    2000 non-null   float64
 3   Sleep_Hours_Per_Day              2000 non-null   float64
 4   Social_Hours_Per_Day             2000 non-null   float64
 5   Physical_Activity_Hours_Per_Day  2000 non-null   float64
 6   GPA                              2000 non-null   float64
 7   Stress_Level                     2000 non-null   object 
dtypes: float64(6), int64(1), object(1)
memory usage: 125.1+ KB
None

[INFO] Estadísticas descriptivas:
         Student_ID  Study_Hours_Per_Day  ...          GPA  Stress_Level
count   2000.000000          2000.000000  ...  2000.000000          2000
unique          NaN                  NaN  ...          NaN             3
top             NaN                  NaN  ...          NaN          High
freq            NaN                  NaN  ...          NaN           940
mean    1000.500000             7.475800  ...     3.115960           NaN
std      577.494589             1.423888  ...     0.298674           NaN
min        1.000000             5.000000  ...     2.240000           NaN
25%      500.750000             6.300000  ...     2.900000           NaN
50%     1000.500000             7.400000  ...     3.110000           NaN
75%     1500.250000             8.700000  ...     3.330000           NaN
max     2000.000000            10.000000  ...     4.000000           NaN

[11 rows x 8 columns]

[INFO] Valores faltantes:
Student_ID                         0
Study_Hours_Per_Day                0
Extracurricular_Hours_Per_Day      0
Sleep_Hours_Per_Day                0
Social_Hours_Per_Day               0
Physical_Activity_Hours_Per_Day    0
GPA                                0
Stress_Level                       0
dtype: int64

[INFO] Distribución de la columna objetivo (Stress_Level):
Stress_Level
High        940
Moderate    664
Low         396
Name: count, dtype: int64

[INFO] Generando histogramas de distribución...
[OK] Grafica de distribuciones guardada

======================================================================
PASO 2: LIMPIEZA Y TRANSFORMACIÓN DEL DATASET
======================================================================

[INFO] Análisis de valores anomalos en columnas numéricas:

Student_ID:
  Minimo: 1
  Maximo: 2000
  Valores nulos: 0

Study_Hours_Per_Day:
  Minimo: 5.0
  Maximo: 10.0
  Valores nulos: 0

Extracurricular_Hours_Per_Day:
  Minimo: 0.0
  Maximo: 4.0
  Valores nulos: 0

Sleep_Hours_Per_Day:
  Minimo: 5.0
  Maximo: 10.0
  Valores nulos: 0

Social_Hours_Per_Day:
  Minimo: 0.0
  Maximo: 6.0
  Valores nulos: 0

Physical_Activity_Hours_Per_Day:
  Minimo: 0.0
  Maximo: 13.0
  Valores nulos: 0

GPA:
  Minimo: 2.24
  Maximo: 4.0
  Valores nulos: 0

[INFO] Eliminando columna Student_ID (es solo un identificador)...
[INFO] Eliminando filas con valores nulos...
[INFO] Forma despues de eliminar nulos: (2000, 7)
[INFO] Filas duplicadas: 0

[INFO] Dataset despues de limpieza:
   Study_Hours_Per_Day  Extracurricular_Hours_Per_Day  ...   GPA  Stress_Level
0                  6.9                            3.8  ...  2.99      Moderate
1                  5.3                            3.5  ...  2.75           Low
2                  5.1                            3.9  ...  2.67           Low
3                  6.5                            2.1  ...  2.88      Moderate
4                  8.1                            0.6  ...  3.51          High

[5 rows x 7 columns]
[INFO] Forma final del dataset limpio: (2000, 7)

======================================================================
PASO 3: DEFINIENDO CARACTERÍSTICAS Y OBJETIVO
======================================================================

[INFO] Columnas categoricas: []
[INFO] Columnas numericas: ['Study_Hours_Per_Day', 'Extracurricular_Hours_Per_Day', 'Sleep_Hours_Per_Day', 'Social_Hours_Per_Day', 'Physical_Activity_Hours_Per_Day', 'GPA']
[INFO] Columna objetivo: Stress_Level

[INFO] Caracteristicas (X):
   Study_Hours_Per_Day  Extracurricular_Hours_Per_Day  ...  Physical_Activity_Hours_Per_Day   GPA
0                  6.9                            3.8  ...                              1.8  2.99
1                  5.3                            3.5  ...                              3.0  2.75
2                  5.1                            3.9  ...                              4.6  2.67
3                  6.5                            2.1  ...                              6.5  2.88
4                  8.1                            0.6  ...                              6.6  3.51

[5 rows x 6 columns]
[INFO] Forma de X: (2000, 6)

[INFO] Objetivo (y):
0    Moderate
1         Low
2         Low
3    Moderate
4        High
Name: Stress_Level, dtype: object
[INFO] Forma de y: (2000,)

[INFO] Distribucion de clases en el objetivo:
Stress_Level
High        940
Moderate    664
Low         396
Name: count, dtype: int64
[OK] Grafica de distribucion de clases guardada

======================================================================
TRANSFORMACIÓN DE DATOS - ORDINAL ENCODING
======================================================================

[INFO] No hay columnas categoricas para transformar

======================================================================
CODIFICACIÓN DE VARIABLE OBJETIVO
======================================================================

[INFO] Clases: ['High' 'Low' 'Moderate']
[INFO] Encoding: {'High': 0, 'Low': 1, 'Moderate': 2}

======================================================================
BALANCEO DE CLASES (UNDERSAMPLING)
======================================================================

[INFO] Distribución de clases ANTES del balanceo:
  Clase High: 940
  Clase Low: 396
  Clase Moderate: 664

[INFO] Clase minoritaria tiene 396 muestras
[INFO] Balanceando todas las clases a 396 muestras...
[INFO] Clase High: 940 -> 396 muestras
[INFO] Clase Low: 396 -> 396 muestras
[INFO] Clase Moderate: 664 -> 396 muestras

[OK] Dataset balanceado exitosamente
[INFO] Forma del dataset balanceado: (1188, 6)

[INFO] Distribución de clases DESPUÉS del balanceo:
  Clase High: 396
  Clase Low: 396
  Clase Moderate: 396

======================================================================
DIVISION EN ENTRENAMIENTO (60%), VALIDACIÓN (20%) Y PRUEBA (20%)
======================================================================

[INFO] Tamaño del conjunto de entrenamiento: 712 muestras (59.9%)
[INFO] Tamaño del conjunto de validación: 238 muestras (20.0%)
[INFO] Tamaño del conjunto de prueba: 238 muestras (20.0%)

[INFO] Distribucion de clases en entrenamiento:
  Clase High: 238
  Clase Low: 237
  Clase Moderate: 237

[INFO] Distribucion de clases en validación:
  Clase High: 79
  Clase Low: 79
  Clase Moderate: 80

[INFO] Distribucion de clases en prueba:
  Clase High: 79
  Clase Low: 80
  Clase Moderate: 79

======================================================================
GUARDANDO DATOS PROCESADOS
======================================================================
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados\student_lifestyle_train.csv' guardado (712 muestras)
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados\student_lifestyle_validation.csv' guardado (238 muestras)
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados\student_lifestyle_test.csv' guardado (238 muestras)

======================================================================
ENTRENAMIENTO - RANDOM FOREST CLASSIFIER
======================================================================

[INFO] Configurando modelo Random Forest...
[INFO] Iniciando GridSearchCV para Random Forest...
[INFO] Parametros a explorar: {'n_estimators': [50, 100, 200], 'max_depth': [6, 8, 10, 12], 'min_samples_leaf': [2, 5, 10]}
Fitting 3 folds for each of 36 candidates, totalling 108 fits

[OK] Entrenamiento completado en 7.50 segundos
[INFO] Mejores parametros: {'max_depth': 6, 'min_samples_leaf': 2, 'n_estimators': 50}
[INFO] Mejor score (CV): 0.7725

======================================================================
ENTRENAMIENTO - GRADIENT BOOSTING CLASSIFIER
======================================================================

[INFO] Configurando modelo Gradient Boosting...
[INFO] Iniciando GridSearchCV para Gradient Boosting...
[INFO] Parametros a explorar: {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9], 'learning_rate': [0.01, 0.05, 0.1]}
Fitting 3 folds for each of 36 candidates, totalling 108 fits

[OK] Entrenamiento completado en 18.51 segundos
[INFO] Mejores parametros: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}
[INFO] Mejor score (CV): 0.7697

======================================================================
EVALUACIÓN DE MODELOS
======================================================================

======================================================================
MODELO: Random Forest
======================================================================

[INFO] CONJUNTO DE ENTRENAMIENTO:
  Accuracy:  0.7978
  Precision: 0.8156
  Recall:    0.7978
  F1-Score:  0.7959

[INFO] CONJUNTO DE VALIDACIÓN:
  Accuracy:  0.7437
  Precision: 0.7607
  Recall:    0.7437
  F1-Score:  0.7398

[INFO] CONJUNTO DE PRUEBA:
  Accuracy:  0.7689
  Precision: 0.7838
  Recall:    0.7689
  F1-Score:  0.7658

[INFO] REPORTE DE CLASIFICACIÓN (CONJUNTO DE PRUEBA):
              precision    recall  f1-score   support

        High       0.71      0.85      0.77        79
         Low       0.88      0.61      0.72        80
    Moderate       0.77      0.85      0.81        79

    accuracy                           0.77       238
   macro avg       0.78      0.77      0.77       238
weighted avg       0.78      0.77      0.77       238


[INFO] Matriz de confusión (Entrenamiento):
[[214   7  17]
 [ 44 156  37]
 [ 34   5 198]]

[INFO] Matriz de confusión (Validación):
[[69  1  9]
 [14 46 19]
 [12  6 62]]

[INFO] Matriz de confusión (Prueba):
[[67  4  8]
 [19 49 12]
 [ 9  3 67]]
[OK] Matrices de confusión guardadas: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\figures\confusion_matrices_random_forest.png

======================================================================
MODELO: Gradient Boosting
======================================================================

[INFO] CONJUNTO DE ENTRENAMIENTO:
  Accuracy:  0.7949
  Precision: 0.8114
  Recall:    0.7949
  F1-Score:  0.7934

[INFO] CONJUNTO DE VALIDACIÓN:
  Accuracy:  0.7521
  Precision: 0.7683
  Recall:    0.7521
  F1-Score:  0.7469

[INFO] CONJUNTO DE PRUEBA:
  Accuracy:  0.7815
  Precision: 0.7991
  Recall:    0.7815
  F1-Score:  0.7784

[INFO] REPORTE DE CLASIFICACIÓN (CONJUNTO DE PRUEBA):
              precision    recall  f1-score   support

        High       0.74      0.89      0.80        79
         Low       0.91      0.62      0.74        80
    Moderate       0.75      0.84      0.79        79

    accuracy                           0.78       238
   macro avg       0.80      0.78      0.78       238
weighted avg       0.80      0.78      0.78       238


[INFO] Matriz de confusión (Entrenamiento):
[[214   7  17]
 [ 43 158  36]
 [ 36   7 194]]

[INFO] Matriz de confusión (Validación):
[[72  0  7]
 [16 46 17]
 [12  7 61]]

[INFO] Matriz de confusión (Prueba):
[[70  1  8]
 [16 50 14]
 [ 9  4 66]]
[OK] Matrices de confusión guardadas: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\figures\confusion_matrices_gradient_boosting.png

======================================================================
COMPARACIÓN DE MODELOS
======================================================================

[INFO] Tabla de comparación de métricas:
           Modelo Conjunto  Accuracy  Precision   Recall  F1-Score
    Random Forest    train  0.797753   0.815604 0.797753  0.795928
    Random Forest      val  0.743697   0.760740 0.743697  0.739785
    Random Forest     test  0.768908   0.783844 0.768908  0.765787
Gradient Boosting    train  0.794944   0.811354 0.794944  0.793449
Gradient Boosting      val  0.752101   0.768310 0.752101  0.746913
Gradient Boosting     test  0.781513   0.799108 0.781513  0.778427

[OK] Gráfica de comparación de modelos (MEJORADA) guardada con éxito

======================================================================
IMPORTANCIA DE CARACTERÍSTICAS
======================================================================

[INFO] Top 10 caracteristicas mas importantes (Random Forest):
                        feature  importance
            Study_Hours_Per_Day    0.494755
            Sleep_Hours_Per_Day    0.166414
                            GPA    0.163333
Physical_Activity_Hours_Per_Day    0.079493
           Social_Hours_Per_Day    0.051391
  Extracurricular_Hours_Per_Day    0.044614

[INFO] Top 10 caracteristicas mas importantes (Gradient Boosting):
                        feature  importance
            Study_Hours_Per_Day    0.678855
            Sleep_Hours_Per_Day    0.233267
                            GPA    0.040743
  Extracurricular_Hours_Per_Day    0.019906
Physical_Activity_Hours_Per_Day    0.018037
           Social_Hours_Per_Day    0.009192
[OK] Grafica de importancia de caracteristicas guardada

======================================================================
VISUALIZACIÓN DE ÁRBOLES DE DECISIÓN
======================================================================

[INFO] Graficando primer árbol de Random Forest...
[OK] Árbol de Random Forest guardado

[INFO] Graficando primer árbol de Gradient Boosting...
[OK] Árbol de Gradient Boosting guardado

[INFO] Graficando primeros 4 árboles de Random Forest...
[OK] Múltiples árboles de Random Forest guardados

======================================================================
GUARDANDO RESULTADOS
======================================================================
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\results\training_results.json' guardado
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\results\model_comparison.csv' guardado

======================================================================
RESUMEN FINAL
======================================================================

[INFO] Dataset original: 2000 filas, 8 columnas
[INFO] Dataset procesado: 1188 filas, 6 caracteristicas + 1 objetivo

[INFO] Division de datos:
  - Entrenamiento (60%): (712, 6)
  - Validación (20%):    (238, 6)
  - Prueba (20%):        (238, 6)

[INFO] Caracteristicas utilizadas:
  1. Study_Hours_Per_Day
  2. Extracurricular_Hours_Per_Day
  3. Sleep_Hours_Per_Day
  4. Social_Hours_Per_Day
  5. Physical_Activity_Hours_Per_Day
  6. GPA

[INFO] Variables objetivo (Stress_Level):
  0. High
  1. Low
  2. Moderate

[INFO] Mejores parametros:
  Random Forest: {'max_depth': 6, 'min_samples_leaf': 2, 'n_estimators': 50}
  Gradient Boosting: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}

[INFO] Mejor performance en conjunto de PRUEBA:
  Modelo: Gradient Boosting
  Accuracy: 0.7815

[INFO] Archivos guardados en:
  - Datos procesados: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados/
  - Figuras: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\figures/
  - Resultados: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\results/

======================================================================
[OK] PROCESAMIENTO COMPLETADO EXITOSAMENTE
======================================================================
