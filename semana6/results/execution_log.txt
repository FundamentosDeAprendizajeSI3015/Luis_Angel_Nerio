======================================================================
INICIALIZACIÓN DEL SISTEMA
======================================================================
[OK] Carpetas creadas exitosamente
[INFO] Datos: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados
[INFO] Figuras: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\figures
[INFO] Resultados: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\results

======================================================================
PASO 1: CARGANDO LOS DATOS
======================================================================
[INFO] Buscando dataset en: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\student_lifestyle_dataset.csv
[OK] Dataset cargado con éxito
[INFO] Forma del dataset: (2000, 8)

[INFO] Primeras filas:
   Student_ID  Study_Hours_Per_Day  ...   GPA  Stress_Level
0           1                  6.9  ...  2.99      Moderate
1           2                  5.3  ...  2.75           Low
2           3                  5.1  ...  2.67           Low
3           4                  6.5  ...  2.88      Moderate
4           5                  8.1  ...  3.51          High

[5 rows x 8 columns]

======================================================================
EXPLORACIÓN DEL DATASET
======================================================================

[INFO] Información general del dataset:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2000 entries, 0 to 1999
Data columns (total 8 columns):
 #   Column                           Non-Null Count  Dtype  
---  ------                           --------------  -----  
 0   Student_ID                       2000 non-null   int64  
 1   Study_Hours_Per_Day              2000 non-null   float64
 2   Extracurricular_Hours_Per_Day    2000 non-null   float64
 3   Sleep_Hours_Per_Day              2000 non-null   float64
 4   Social_Hours_Per_Day             2000 non-null   float64
 5   Physical_Activity_Hours_Per_Day  2000 non-null   float64
 6   GPA                              2000 non-null   float64
 7   Stress_Level                     2000 non-null   object 
dtypes: float64(6), int64(1), object(1)
memory usage: 125.1+ KB
None

[INFO] Estadísticas descriptivas:
         Student_ID  Study_Hours_Per_Day  ...          GPA  Stress_Level
count   2000.000000          2000.000000  ...  2000.000000          2000
unique          NaN                  NaN  ...          NaN             3
top             NaN                  NaN  ...          NaN          High
freq            NaN                  NaN  ...          NaN          1029
mean    1000.500000             7.475800  ...     3.115960           NaN
std      577.494589             1.423888  ...     0.298674           NaN
min        1.000000             5.000000  ...     2.240000           NaN
25%      500.750000             6.300000  ...     2.900000           NaN
50%     1000.500000             7.400000  ...     3.110000           NaN
75%     1500.250000             8.700000  ...     3.330000           NaN
max     2000.000000            10.000000  ...     4.000000           NaN

[11 rows x 8 columns]

[INFO] Valores faltantes:
Student_ID                         0
Study_Hours_Per_Day                0
Extracurricular_Hours_Per_Day      0
Sleep_Hours_Per_Day                0
Social_Hours_Per_Day               0
Physical_Activity_Hours_Per_Day    0
GPA                                0
Stress_Level                       0
dtype: int64

[INFO] Distribución de la columna objetivo (Stress_Level):
Stress_Level
High        1029
Moderate     674
Low          297
Name: count, dtype: int64

[INFO] Generando histogramas de distribución...
[OK] Grafica de distribuciones guardada

======================================================================
PASO 2: LIMPIEZA Y TRANSFORMACIÓN DEL DATASET
======================================================================

[INFO] Análisis de valores anomalos en columnas numéricas:

Student_ID:
  Minimo: 1
  Maximo: 2000
  Valores nulos: 0

Study_Hours_Per_Day:
  Minimo: 5.0
  Maximo: 10.0
  Valores nulos: 0

Extracurricular_Hours_Per_Day:
  Minimo: 0.0
  Maximo: 4.0
  Valores nulos: 0

Sleep_Hours_Per_Day:
  Minimo: 5.0
  Maximo: 10.0
  Valores nulos: 0

Social_Hours_Per_Day:
  Minimo: 0.0
  Maximo: 6.0
  Valores nulos: 0

Physical_Activity_Hours_Per_Day:
  Minimo: 0.0
  Maximo: 13.0
  Valores nulos: 0

GPA:
  Minimo: 2.24
  Maximo: 4.0
  Valores nulos: 0

[INFO] Eliminando columna Student_ID (es solo un identificador)...
[INFO] Eliminando filas con valores nulos...
[INFO] Forma despues de eliminar nulos: (2000, 7)
[INFO] Filas duplicadas: 0

[INFO] Dataset despues de limpieza:
   Study_Hours_Per_Day  Extracurricular_Hours_Per_Day  ...   GPA  Stress_Level
0                  6.9                            3.8  ...  2.99      Moderate
1                  5.3                            3.5  ...  2.75           Low
2                  5.1                            3.9  ...  2.67           Low
3                  6.5                            2.1  ...  2.88      Moderate
4                  8.1                            0.6  ...  3.51          High

[5 rows x 7 columns]
[INFO] Forma final del dataset limpio: (2000, 7)

======================================================================
PASO 3: DEFINIENDO CARACTERÍSTICAS Y OBJETIVO
======================================================================

[INFO] Columnas categoricas: []
[INFO] Columnas numericas: ['Study_Hours_Per_Day', 'Extracurricular_Hours_Per_Day', 'Sleep_Hours_Per_Day', 'Social_Hours_Per_Day', 'Physical_Activity_Hours_Per_Day', 'GPA']
[INFO] Columna objetivo: Stress_Level

[INFO] Caracteristicas (X):
   Study_Hours_Per_Day  Extracurricular_Hours_Per_Day  ...  Physical_Activity_Hours_Per_Day   GPA
0                  6.9                            3.8  ...                              1.8  2.99
1                  5.3                            3.5  ...                              3.0  2.75
2                  5.1                            3.9  ...                              4.6  2.67
3                  6.5                            2.1  ...                              6.5  2.88
4                  8.1                            0.6  ...                              6.6  3.51

[5 rows x 6 columns]
[INFO] Forma de X: (2000, 6)

[INFO] Objetivo (y):
0    Moderate
1         Low
2         Low
3    Moderate
4        High
Name: Stress_Level, dtype: object
[INFO] Forma de y: (2000,)

[INFO] Distribucion de clases en el objetivo:
Stress_Level
High        1029
Moderate     674
Low          297
Name: count, dtype: int64
[OK] Grafica de distribucion de clases guardada

======================================================================
TRANSFORMACIÓN DE DATOS - ORDINAL ENCODING
======================================================================

[INFO] No hay columnas categoricas para transformar

======================================================================
CODIFICACIÓN DE VARIABLE OBJETIVO
======================================================================

[INFO] Clases: ['High' 'Low' 'Moderate']
[INFO] Encoding: {'High': 0, 'Low': 1, 'Moderate': 2}

======================================================================
BALANCEO DE CLASES (UNDERSAMPLING)
======================================================================

[INFO] Distribución de clases ANTES del balanceo:
  Clase High: 1029
  Clase Low: 297
  Clase Moderate: 674

[INFO] Clase minoritaria tiene 297 muestras
[INFO] Balanceando todas las clases a 297 muestras...
[INFO] Clase High: 1029 -> 297 muestras
[INFO] Clase Low: 297 -> 297 muestras
[INFO] Clase Moderate: 674 -> 297 muestras

[OK] Dataset balanceado exitosamente
[INFO] Forma del dataset balanceado: (891, 6)

[INFO] Distribución de clases DESPUÉS del balanceo:
  Clase High: 297
  Clase Low: 297
  Clase Moderate: 297

======================================================================
DIVISION EN ENTRENAMIENTO (60%), VALIDACIÓN (20%) Y PRUEBA (20%)
======================================================================

[INFO] Tamaño del conjunto de entrenamiento: 534 muestras (59.9%)
[INFO] Tamaño del conjunto de validación: 178 muestras (20.0%)
[INFO] Tamaño del conjunto de prueba: 179 muestras (20.1%)

[INFO] Distribucion de clases en entrenamiento:
  Clase High: 178
  Clase Low: 178
  Clase Moderate: 178

[INFO] Distribucion de clases en validación:
  Clase High: 60
  Clase Low: 59
  Clase Moderate: 59

[INFO] Distribucion de clases en prueba:
  Clase High: 59
  Clase Low: 60
  Clase Moderate: 60

======================================================================
GUARDANDO DATOS PROCESADOS
======================================================================
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados\student_lifestyle_train.csv' guardado (534 muestras)
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados\student_lifestyle_validation.csv' guardado (178 muestras)
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados\student_lifestyle_test.csv' guardado (179 muestras)

======================================================================
ENTRENAMIENTO - RANDOM FOREST CLASSIFIER
======================================================================

[INFO] Configurando modelo Random Forest...
[INFO] Iniciando GridSearchCV para Random Forest...
[INFO] Parametros a explorar: {'n_estimators': [50, 100, 200], 'max_depth': [6, 8, 10, 12], 'min_samples_leaf': [2, 5, 10]}
Fitting 3 folds for each of 36 candidates, totalling 108 fits

[OK] Entrenamiento completado en 9.62 segundos
[INFO] Mejores parametros: {'max_depth': 6, 'min_samples_leaf': 2, 'n_estimators': 100}
[INFO] Mejor score (CV): 1.0000

======================================================================
ENTRENAMIENTO - GRADIENT BOOSTING CLASSIFIER
======================================================================

[INFO] Configurando modelo Gradient Boosting...
[INFO] Iniciando GridSearchCV para Gradient Boosting...
[INFO] Parametros a explorar: {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 9], 'learning_rate': [0.01, 0.05, 0.1]}
Fitting 3 folds for each of 36 candidates, totalling 108 fits

[OK] Entrenamiento completado en 5.33 segundos
[INFO] Mejores parametros: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}
[INFO] Mejor score (CV): 1.0000

======================================================================
EVALUACIÓN DE MODELOS
======================================================================

======================================================================
MODELO: Random Forest
======================================================================

[INFO] CONJUNTO DE ENTRENAMIENTO:
  Accuracy:  1.0000
  Precision: 1.0000
  Recall:    1.0000
  F1-Score:  1.0000

[INFO] CONJUNTO DE VALIDACIÓN:
  Accuracy:  1.0000
  Precision: 1.0000
  Recall:    1.0000
  F1-Score:  1.0000

[INFO] CONJUNTO DE PRUEBA:
  Accuracy:  1.0000
  Precision: 1.0000
  Recall:    1.0000
  F1-Score:  1.0000

[INFO] REPORTE DE CLASIFICACIÓN (CONJUNTO DE PRUEBA):
              precision    recall  f1-score   support

        High       1.00      1.00      1.00        59
         Low       1.00      1.00      1.00        60
    Moderate       1.00      1.00      1.00        60

    accuracy                           1.00       179
   macro avg       1.00      1.00      1.00       179
weighted avg       1.00      1.00      1.00       179


[INFO] Matriz de confusión (Entrenamiento):
[[178   0   0]
 [  0 178   0]
 [  0   0 178]]

[INFO] Matriz de confusión (Validación):
[[60  0  0]
 [ 0 59  0]
 [ 0  0 59]]

[INFO] Matriz de confusión (Prueba):
[[59  0  0]
 [ 0 60  0]
 [ 0  0 60]]
[OK] Matrices de confusión guardadas: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\figures\confusion_matrices_random_forest.png

======================================================================
MODELO: Gradient Boosting
======================================================================

[INFO] CONJUNTO DE ENTRENAMIENTO:
  Accuracy:  1.0000
  Precision: 1.0000
  Recall:    1.0000
  F1-Score:  1.0000

[INFO] CONJUNTO DE VALIDACIÓN:
  Accuracy:  1.0000
  Precision: 1.0000
  Recall:    1.0000
  F1-Score:  1.0000

[INFO] CONJUNTO DE PRUEBA:
  Accuracy:  1.0000
  Precision: 1.0000
  Recall:    1.0000
  F1-Score:  1.0000

[INFO] REPORTE DE CLASIFICACIÓN (CONJUNTO DE PRUEBA):
              precision    recall  f1-score   support

        High       1.00      1.00      1.00        59
         Low       1.00      1.00      1.00        60
    Moderate       1.00      1.00      1.00        60

    accuracy                           1.00       179
   macro avg       1.00      1.00      1.00       179
weighted avg       1.00      1.00      1.00       179


[INFO] Matriz de confusión (Entrenamiento):
[[178   0   0]
 [  0 178   0]
 [  0   0 178]]

[INFO] Matriz de confusión (Validación):
[[60  0  0]
 [ 0 59  0]
 [ 0  0 59]]

[INFO] Matriz de confusión (Prueba):
[[59  0  0]
 [ 0 60  0]
 [ 0  0 60]]
[OK] Matrices de confusión guardadas: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\figures\confusion_matrices_gradient_boosting.png

======================================================================
COMPARACIÓN DE MODELOS
======================================================================

[INFO] Tabla de comparación de métricas:
           Modelo Conjunto  Accuracy  Precision  Recall  F1-Score
    Random Forest    train       1.0        1.0     1.0       1.0
    Random Forest      val       1.0        1.0     1.0       1.0
    Random Forest     test       1.0        1.0     1.0       1.0
Gradient Boosting    train       1.0        1.0     1.0       1.0
Gradient Boosting      val       1.0        1.0     1.0       1.0
Gradient Boosting     test       1.0        1.0     1.0       1.0

[OK] Gráfica de comparación de modelos (MEJORADA) guardada con éxito

======================================================================
IMPORTANCIA DE CARACTERÍSTICAS
======================================================================

[INFO] Top 10 caracteristicas mas importantes (Random Forest):
                        feature  importance
            Study_Hours_Per_Day    0.625600
            Sleep_Hours_Per_Day    0.167664
                            GPA    0.118709
Physical_Activity_Hours_Per_Day    0.045415
           Social_Hours_Per_Day    0.023653
  Extracurricular_Hours_Per_Day    0.018959

[INFO] Top 10 caracteristicas mas importantes (Gradient Boosting):
                        feature   importance
            Study_Hours_Per_Day 7.696361e-01
            Sleep_Hours_Per_Day 2.303639e-01
                            GPA 7.034694e-16
Physical_Activity_Hours_Per_Day 5.779628e-16
  Extracurricular_Hours_Per_Day 4.154707e-16
           Social_Hours_Per_Day 2.195652e-16
[OK] Grafica de importancia de caracteristicas guardada

======================================================================
VISUALIZACIÓN DE ÁRBOLES DE DECISIÓN
======================================================================

[INFO] Graficando primer árbol de Random Forest...
[OK] Árbol de Random Forest guardado

[INFO] Graficando primer árbol de Gradient Boosting...
[OK] Árbol de Gradient Boosting guardado

[INFO] Graficando primeros 4 árboles de Random Forest...
[OK] Múltiples árboles de Random Forest guardados

======================================================================
GUARDANDO RESULTADOS
======================================================================
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\results\training_results.json' guardado
[OK] Archivo 'c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\results\model_comparison.csv' guardado

======================================================================
RESUMEN FINAL
======================================================================

[INFO] Dataset original: 2000 filas, 8 columnas
[INFO] Dataset procesado: 891 filas, 6 caracteristicas + 1 objetivo

[INFO] Division de datos:
  - Entrenamiento (60%): (534, 6)
  - Validación (20%):    (178, 6)
  - Prueba (20%):        (179, 6)

[INFO] Caracteristicas utilizadas:
  1. Study_Hours_Per_Day
  2. Extracurricular_Hours_Per_Day
  3. Sleep_Hours_Per_Day
  4. Social_Hours_Per_Day
  5. Physical_Activity_Hours_Per_Day
  6. GPA

[INFO] Variables objetivo (Stress_Level):
  0. High
  1. Low
  2. Moderate

[INFO] Mejores parametros:
  Random Forest: {'max_depth': 6, 'min_samples_leaf': 2, 'n_estimators': 100}
  Gradient Boosting: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}

[INFO] Mejor performance en conjunto de PRUEBA:
  Modelo: Random Forest
  Accuracy: 1.0000

[INFO] Archivos guardados en:
  - Datos procesados: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\datos_procesados/
  - Figuras: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\figures/
  - Resultados: c:\Users\luisa\OneDrive\Documentos\Universidad\Aprendizaje automatico\git hub\Luis_Angel_Nerio\semana6\results/

======================================================================
[OK] PROCESAMIENTO COMPLETADO EXITOSAMENTE
======================================================================
