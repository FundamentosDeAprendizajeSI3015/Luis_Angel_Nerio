

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import sys

# Cargar el DataFrame del Titanic
df = pd.read_csv('Titanic-Dataset.csv')

# ============================================
# TRANSFORMACI√ìN DE COLUMNAS - ENCODING
# ============================================

print("\n" + "="*60)
print("TRANSFORMACI√ìN DE COLUMNAS - ENCODING")
print("="*60)

# Trabajaremos con una copia del DataFrame original del Titanic
df_encoding = df.copy()

# --- ONE HOT ENCODING ---
print("\n--- 1. ONE HOT ENCODING ---")
print("Transforma variables categ√≥ricas en columnas binarias (0 o 1)")
print("\nColumnas categ√≥ricas originales: Sex, Embarked")

# Mostrar distribuci√≥n original
print("\nDistribuci√≥n de 'Sex':")
print(df_encoding['Sex'].value_counts())
print("\nDistribuci√≥n de 'Embarked':")
print(df_encoding['Embarked'].value_counts())

# Aplicar One-Hot Encoding a las columnas Sex y Embarked
df_one_hot = pd.get_dummies(df_encoding, columns=['Sex', 'Embarked'], prefix=['Sex', 'Embarked'])

print("\nDataFrame despu√©s de One-Hot Encoding (primeras 5 filas):")
print(df_one_hot[['PassengerId', 'Name', 'Sex_female', 'Sex_male', 
                   'Embarked_C', 'Embarked_Q', 'Embarked_S']].head())

print(f"\nColumnas agregadas: {[col for col in df_one_hot.columns if col.startswith(('Sex_', 'Embarked_'))]}")
print(f"Forma del DataFrame Original: {df_encoding.shape}")
print(f"Forma del DataFrame con One-Hot: {df_one_hot.shape}")


# --- LABEL ENCODING ---
print("\n" + "="*60)
print("--- 2. LABEL ENCODING ---")
print("Asigna un n√∫mero entero √∫nico a cada categor√≠a")

from sklearn.preprocessing import LabelEncoder

# Crear una copia para Label Encoding
df_label = df_encoding.copy()

# Aplicar Label Encoding a la columna 'Sex'
le_sex = LabelEncoder()
df_label['Sex_Encoded'] = le_sex.fit_transform(df_label['Sex'])

print("\nLabel Encoding para 'Sex':")
print(f"Categor√≠as originales: {le_sex.classes_}")
print(f"Mapeo: {dict(zip(le_sex.classes_, le_sex.transform(le_sex.classes_)))}")
print("\nComparaci√≥n:")
print(df_label[['PassengerId', 'Name', 'Sex', 'Sex_Encoded']].head(10))

# Aplicar Label Encoding a la columna 'Embarked' (eliminando nulos primero)
df_label_embarked = df_label[df_label['Embarked'].notna()].copy()
le_embarked = LabelEncoder()
df_label_embarked['Embarked_Encoded'] = le_embarked.fit_transform(df_label_embarked['Embarked'])

print("\nLabel Encoding para 'Embarked':")
print(f"Categor√≠as originales: {le_embarked.classes_}")
print(f"Mapeo: {dict(zip(le_embarked.classes_, le_embarked.transform(le_embarked.classes_)))}")
print("\nComparaci√≥n:")
print(df_label_embarked[['PassengerId', 'Name', 'Embarked', 'Embarked_Encoded']].head(10))


# --- BINARY ENCODING ---
print("\n" + "="*60)
print("--- 3. BINARY ENCODING ---")
print("Convierte categor√≠as a representaci√≥n binaria (m√°s eficiente para muchas categor√≠as)")

# Verificar si category_encoders est√° instalado, si no, intentar instalarlo
try:
    import category_encoders as ce
except ImportError:
    print("\nInstalando category_encoders...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "category_encoders", "-q"])
    import category_encoders as ce

# Crear una copia para Binary Encoding
df_binary = df_encoding.copy()

# Aplicar Binary Encoding a la columna 'Embarked'
# Primero llenamos los nulos con 'Unknown'
df_binary['Embarked'] = df_binary['Embarked'].fillna('Unknown')

encoder_embarked = ce.BinaryEncoder(cols=['Embarked'])
df_binary_embarked = encoder_embarked.fit_transform(df_binary['Embarked'])

print("\nBinary Encoding para 'Embarked':")
print(f"Categor√≠as originales: C, Q, S, Unknown")
print("\nColumnas generadas por Binary Encoding:")
print(df_binary_embarked.head(10))

# Concatenar con el dataframe original
df_binary = pd.concat([df_binary, df_binary_embarked], axis=1)

print("\nDataFrame con Binary Encoding (muestra):")
print(df_binary[['PassengerId', 'Name', 'Embarked'] + list(df_binary_embarked.columns)].head(10))


# Aplicar Binary Encoding a la columna 'Pclass' (aunque es num√©rica, la trataremos como categ√≥rica)
encoder_pclass = ce.BinaryEncoder(cols=['Pclass'])
df_binary_pclass = encoder_pclass.fit_transform(df_binary[['Pclass']])

print("\nBinary Encoding para 'Pclass':")
print(f"Categor√≠as originales: 1, 2, 3")
print("\nColumnas generadas:")
print(df_binary_pclass.head(10))


# --- COMPARACI√ìN DE T√âCNICAS ---
print("\n" + "="*60)
print("COMPARACI√ìN DE T√âCNICAS DE ENCODING")
print("="*60)

print("\n1. ONE-HOT ENCODING:")
print("   ‚úì Ventajas: F√°cil de interpretar, no asume orden entre categor√≠as")
print("   ‚úó Desventajas: Aumenta mucho las dimensiones con muchas categor√≠as")
print(f"   Ejemplo: 'Embarked' (3 categor√≠as) ‚Üí 3 columnas binarias")

print("\n2. LABEL ENCODING:")
print("   ‚úì Ventajas: Muy simple, no aumenta dimensiones")
print("   ‚úó Desventajas: Asume orden entre categor√≠as (puede confundir al modelo)")
print(f"   Ejemplo: 'Embarked' (3 categor√≠as) ‚Üí 1 columna con valores 0, 1, 2")

print("\n3. BINARY ENCODING:")
print("   ‚úì Ventajas: M√°s eficiente que One-Hot, mantiene baja dimensionalidad")
print("   ‚úó Desventajas: Menos interpretable, requiere librer√≠a adicional")
print(f"   Ejemplo: 'Embarked' (4 categor√≠as con Unknown) ‚Üí 2 columnas binarias")

print("\n" + "="*60)
print("RESUMEN DE SHAPES")
print("="*60)
print(f"DataFrame Original: {df_encoding.shape}")
print(f"Con One-Hot Encoding: {df_one_hot.shape}")
print(f"Con Label Encoding: {df_label.shape} (mismo tama√±o, solo nuevas columnas)")
print(f"Con Binary Encoding: {df_binary.shape}")

print("\n--- Transformaci√≥n de Columnas Completada ---")


# ============================================
# AN√ÅLISIS DE CORRELACI√ìN Y SELECCI√ìN DE COLUMNAS
# ============================================

print("\n" + "="*60)
print("AN√ÅLISIS DE CORRELACI√ìN ENTRE COLUMNAS")
print("="*60)

# Seleccionar solo las columnas num√©ricas del DataFrame original
columnas_numericas = ['Age', 'Fare', 'SibSp', 'Parch', 'Pclass', 'Survived']
df_correlacion = df[columnas_numericas].copy()

# Eliminar filas con valores nulos para el an√°lisis
print(f"\nFilas originales: {len(df)}")
df_correlacion = df_correlacion.dropna()
print(f"Filas despu√©s de eliminar nulos: {len(df_correlacion)}")

# --- Correlaci√≥n entre dos columnas espec√≠ficas ---
print("\n--- Correlaci√≥n entre columnas espec√≠ficas ---")

# Correlaci√≥n Age vs Fare
corr_age_fare = df_correlacion['Age'].corr(df_correlacion['Fare'])
print(f"Correlaci√≥n entre Age y Fare: {corr_age_fare:.4f}")

# Correlaci√≥n SibSp vs Parch
corr_sibsp_parch = df_correlacion['SibSp'].corr(df_correlacion['Parch'])
print(f"Correlaci√≥n entre SibSp y Parch: {corr_sibsp_parch:.4f}")

# Correlaci√≥n Pclass vs Fare
corr_pclass_fare = df_correlacion['Pclass'].corr(df_correlacion['Fare'])
print(f"Correlaci√≥n entre Pclass y Fare: {corr_pclass_fare:.4f}")

# Correlaci√≥n Pclass vs Survived
corr_pclass_survived = df_correlacion['Pclass'].corr(df_correlacion['Survived'])
print(f"Correlaci√≥n entre Pclass y Survived: {corr_pclass_survived:.4f}")


# --- Matriz de Correlaci√≥n Completa ---
print("\n--- Matriz de Correlaci√≥n Completa ---")
matriz_corr = df_correlacion.corr()
print("\nMatriz de correlaci√≥n:")
print(matriz_corr.round(3))


# --- Visualizaci√≥n: Mapa de Calor de Correlaciones ---
plt.figure(figsize=(10, 8))
sns.heatmap(matriz_corr, annot=True, cmap='coolwarm', center=0,
            fmt='.3f', linewidths=1, square=True, 
            cbar_kws={'label': 'Coeficiente de Correlaci√≥n'},
            vmin=-1, vmax=1)
plt.title('Mapa de Calor de Correlaciones - Dataset Titanic', fontsize=16, pad=20)
plt.tight_layout()
plt.savefig('09_heatmap_correlaciones.png', dpi=300, bbox_inches='tight')
plt.show()


# --- An√°lisis de Correlaci√≥n con Pearson y Spearman ---
print("\n" + "="*60)
print("COMPARACI√ìN: CORRELACI√ìN PEARSON VS SPEARMAN")
print("="*60)

print("\nPearson: Mide relaciones LINEALES")
print("Spearman: Mide relaciones MONOT√ìNICAS (lineales o no)")

# Comparar ambos m√©todos para algunas variables clave
print("\n--- Age vs Fare ---")
pearson_af = df_correlacion['Age'].corr(df_correlacion['Fare'], method='pearson')
spearman_af = df_correlacion['Age'].corr(df_correlacion['Fare'], method='spearman')
print(f"Pearson:  {pearson_af:.4f}")
print(f"Spearman: {spearman_af:.4f}")

print("\n--- Pclass vs Fare ---")
pearson_pf = df_correlacion['Pclass'].corr(df_correlacion['Fare'], method='pearson')
spearman_pf = df_correlacion['Pclass'].corr(df_correlacion['Fare'], method='spearman')
print(f"Pearson:  {pearson_pf:.4f}")
print(f"Spearman: {spearman_pf:.4f}")

print("\n--- SibSp vs Parch ---")
pearson_sp = df_correlacion['SibSp'].corr(df_correlacion['Parch'], method='pearson')
spearman_sp = df_correlacion['SibSp'].corr(df_correlacion['Parch'], method='spearman')
print(f"Pearson:  {pearson_sp:.4f}")
print(f"Spearman: {spearman_sp:.4f}")


# --- Identificar columnas altamente correlacionadas ---
print("\n" + "="*60)
print("IDENTIFICACI√ìN DE COLUMNAS ALTAMENTE CORRELACIONADAS")
print("="*60)

# Definir umbral para correlaci√≥n alta (t√≠picamente > 0.7 o < -0.7)
umbral_correlacion = 0.7

print(f"\nUmbral de correlaci√≥n alta: ¬±{umbral_correlacion}")
print("\nPares de variables con correlaci√≥n alta:")

# Buscar correlaciones altas (excluyendo la diagonal)
encontrado = False
for i in range(len(matriz_corr.columns)):
    for j in range(i+1, len(matriz_corr.columns)):
        corr_value = matriz_corr.iloc[i, j]
        if abs(corr_value) >= umbral_correlacion:
            var1 = matriz_corr.columns[i]
            var2 = matriz_corr.columns[j]
            print(f"  ‚Ä¢ {var1} vs {var2}: {corr_value:.4f}")
            encontrado = True

if not encontrado:
    print(f"  No se encontraron pares de variables con correlaci√≥n >= {umbral_correlacion}")


# --- Recomendaciones para eliminaci√≥n de columnas ---
print("\n" + "="*60)
print("RECOMENDACIONES PARA ELIMINACI√ìN DE COLUMNAS")
print("="*60)

print("\nüìä CRITERIOS DE DECISI√ìN:")
print("  ‚Ä¢ Correlaci√≥n alta (|r| > 0.7): Considerar eliminar una de las dos")
print("  ‚Ä¢ Correlaci√≥n moderada (0.5 < |r| < 0.7): Evaluar seg√∫n contexto")
print("  ‚Ä¢ Correlaci√≥n baja (|r| < 0.5): Mantener ambas columnas")

print("\nüîç AN√ÅLISIS DEL DATASET TITANIC:")

# Evaluar cada par de correlaciones
correlaciones_importantes = []
for i in range(len(matriz_corr.columns)):
    for j in range(i+1, len(matriz_corr.columns)):
        corr_value = abs(matriz_corr.iloc[i, j])
        if corr_value >= 0.5:  # Correlaci√≥n moderada o alta
            var1 = matriz_corr.columns[i]
            var2 = matriz_corr.columns[j]
            correlaciones_importantes.append((var1, var2, matriz_corr.iloc[i, j]))

if correlaciones_importantes:
    print("\nPares con correlaci√≥n moderada o alta:")
    for var1, var2, corr_val in sorted(correlaciones_importantes, key=lambda x: abs(x[2]), reverse=True):
        if abs(corr_val) >= 0.7:
            nivel = "‚ö†Ô∏è ALTA"
        else:
            nivel = "‚ö° MODERADA"
        print(f"  {nivel} - {var1} vs {var2}: {corr_val:.4f}")
else:
    print("\nNo hay pares con correlaci√≥n moderada o alta.")

print("\nüí° RECOMENDACIONES ESPEC√çFICAS:")

# Analizar correlaciones espec√≠ficas del Titanic
if abs(matriz_corr.loc['Pclass', 'Fare']) >= 0.5:
    print(f"\n  1. Pclass vs Fare (r={matriz_corr.loc['Pclass', 'Fare']:.3f}):")
    print("     ‚Üí Correlaci√≥n moderada/alta NEGATIVA")
    print("     ‚Üí A mayor clase (3), menor tarifa")
    print("     ‚Üí RECOMENDACI√ìN: MANTENER ambas")
    print("       ‚Ä¢ Pclass: Variable categ√≥rica ordinal importante")
    print("       ‚Ä¢ Fare: Variable continua con informaci√≥n √∫nica")

if abs(matriz_corr.loc['SibSp', 'Parch']) >= 0.3:
    print(f"\n  2. SibSp vs Parch (r={matriz_corr.loc['SibSp', 'Parch']:.3f}):")
    print("     ‚Üí Correlaci√≥n baja/moderada POSITIVA")
    print("     ‚Üí Ambas relacionadas con tama√±o de familia")
    print("     ‚Üí RECOMENDACI√ìN: MANTENER ambas")
    print("       ‚Ä¢ Representan relaciones familiares diferentes")
    print("       ‚Ä¢ O crear una nueva variable 'FamilySize' = SibSp + Parch + 1")

if abs(matriz_corr.loc['Age', 'Fare']) < 0.2:
    print(f"\n  3. Age vs Fare (r={matriz_corr.loc['Age', 'Fare']:.3f}):")
    print("     ‚Üí Correlaci√≥n MUY BAJA")
    print("     ‚Üí Variables independientes entre s√≠")
    print("     ‚Üí RECOMENDACI√ìN: MANTENER ambas")

print("\n‚úÖ CONCLUSI√ìN FINAL:")
print("  En el dataset del Titanic, NO hay columnas con correlaci√≥n")
print("  lo suficientemente alta como para recomendar su eliminaci√≥n.")
print("  Todas las variables aportan informaci√≥n √∫nica y valiosa.")

# --- Crear DataFrame con variables seleccionadas (ejemplo) ---
print("\n" + "="*60)
print("CREACI√ìN DE DATASET CON FEATURE ENGINEERING")
print("="*60)

# Crear una nueva variable combinando SibSp y Parch
df_final = df_correlacion.copy()
df_final['FamilySize'] = df_final['SibSp'] + df_final['Parch'] + 1
df_final['IsAlone'] = (df_final['FamilySize'] == 1).astype(int)

print("\nNuevas variables creadas:")
print("  ‚Ä¢ FamilySize: SibSp + Parch + 1")
print("  ‚Ä¢ IsAlone: 1 si viajaba solo, 0 si con familia")

print("\nDataFrame final (primeras 10 filas):")
print(df_final[['Age', 'Fare', 'SibSp', 'Parch', 'FamilySize', 'IsAlone', 'Survived']].head(10))

# Correlaci√≥n de las nuevas variables con Survived
print("\nCorrelaci√≥n de nuevas variables con Survived:")
print(f"  FamilySize vs Survived: {df_final['FamilySize'].corr(df_final['Survived']):.4f}")
print(f"  IsAlone vs Survived:   {df_final['IsAlone'].corr(df_final['Survived']):.4f}")

print("\n--- An√°lisis de Correlaci√≥n Completado ---")

# ============================================
# TRANSFORMACI√ìN LOGAR√çTMICA
# ============================================

print("\n" + "="*60)
print("TRANSFORMACI√ìN LOGAR√çTMICA")
print("="*60)

print("\nüìå ¬øCU√ÅNDO APLICAR TRANSFORMACI√ìN LOGAR√çTMICA?")
print("  ‚Ä¢ Datos con distribuci√≥n muy sesgada (asim√©trica)")
print("  ‚Ä¢ Valores muy dispersos (outliers extremos)")
print("  ‚Ä¢ Diferencias de escala de varios √≥rdenes de magnitud")
print("  ‚Ä¢ Para estabilizar varianza y hacer datos m√°s 'normales'")

# --- An√°lisis de la distribuci√≥n de Fare ---
print("\n" + "="*60)
print("AN√ÅLISIS: ¬øFARE NECESITA TRANSFORMACI√ìN LOGAR√çTMICA?")
print("="*60)

df_fare_analysis = df[df['Fare'] > 0].copy()  # Eliminar Fare = 0 para poder aplicar log
print(f"\nRegistros con Fare > 0: {len(df_fare_analysis)}")
print(f"Registros con Fare = 0: {len(df[df['Fare'] == 0])}")

# Estad√≠sticas de Fare original
print("\nüìä Estad√≠sticas de 'Fare' (Original):")
print(f"  M√≠nimo:  ${df_fare_analysis['Fare'].min():.2f}")
print(f"  Q1:      ${df_fare_analysis['Fare'].quantile(0.25):.2f}")
print(f"  Mediana: ${df_fare_analysis['Fare'].median():.2f}")
print(f"  Q3:      ${df_fare_analysis['Fare'].quantile(0.75):.2f}")
print(f"  M√°ximo:  ${df_fare_analysis['Fare'].max():.2f}")
print(f"  Media:   ${df_fare_analysis['Fare'].mean():.2f}")
print(f"  Desv. Std: ${df_fare_analysis['Fare'].std():.2f}")

# Calcular asimetr√≠a (skewness)
skewness_fare = df_fare_analysis['Fare'].skew()
print(f"\n  Asimetr√≠a (Skewness): {skewness_fare:.3f}")
print("    ‚Üí Skewness > 1: Altamente sesgada a la derecha ‚úì")
print("    ‚Üí ¬°NECESITA TRANSFORMACI√ìN LOGAR√çTMICA!")

# --- Aplicar Transformaci√≥n Logar√≠tmica ---
print("\n--- Aplicando Transformaci√≥n Logar√≠tmica (Log10) ---")

# Aplicar log10 a Fare (solo valores > 0)
df_fare_analysis['Fare_Log10'] = np.log10(df_fare_analysis['Fare'])

# Estad√≠sticas despu√©s de la transformaci√≥n
print("\nüìä Estad√≠sticas de 'Fare_Log10' (Transformado):")
print(f"  M√≠nimo:  {df_fare_analysis['Fare_Log10'].min():.3f}")
print(f"  Q1:      {df_fare_analysis['Fare_Log10'].quantile(0.25):.3f}")
print(f"  Mediana: {df_fare_analysis['Fare_Log10'].median():.3f}")
print(f"  Q3:      {df_fare_analysis['Fare_Log10'].quantile(0.75):.3f}")
print(f"  M√°ximo:  {df_fare_analysis['Fare_Log10'].max():.3f}")
print(f"  Media:   {df_fare_analysis['Fare_Log10'].mean():.3f}")
print(f"  Desv. Std: {df_fare_analysis['Fare_Log10'].std():.3f}")

skewness_fare_log = df_fare_analysis['Fare_Log10'].skew()
print(f"\n  Asimetr√≠a (Skewness): {skewness_fare_log:.3f}")
print("    ‚Üí Skewness reducida significativamente ‚úì")
print("    ‚Üí Distribuci√≥n m√°s sim√©trica y 'normal'")

# --- Visualizaci√≥n: Antes y Despu√©s ---
print("\n--- Generando Visualizaci√≥n Comparativa ---")

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Histograma Fare Original
sns.histplot(df_fare_analysis['Fare'], bins=50, kde=True, ax=axes[0, 0], color='skyblue')
axes[0, 0].set_title('Distribuci√≥n Original de Fare', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Tarifa ($)', fontsize=11)
axes[0, 0].set_ylabel('Frecuencia', fontsize=11)
axes[0, 0].axvline(df_fare_analysis['Fare'].mean(), color='red', linestyle='--', 
                   label=f'Media: ${df_fare_analysis["Fare"].mean():.2f}')
axes[0, 0].axvline(df_fare_analysis['Fare'].median(), color='green', linestyle='-', 
                   label=f'Mediana: ${df_fare_analysis["Fare"].median():.2f}')
axes[0, 0].legend()
axes[0, 0].grid(axis='y', alpha=0.3)

# 2. Histograma Fare Logar√≠tmico
sns.histplot(df_fare_analysis['Fare_Log10'], bins=30, kde=True, ax=axes[0, 1], color='salmon')
axes[0, 1].set_title('Distribuci√≥n Transformada (Log10)', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Log10(Tarifa)', fontsize=11)
axes[0, 1].set_ylabel('Frecuencia', fontsize=11)
axes[0, 1].axvline(df_fare_analysis['Fare_Log10'].mean(), color='red', linestyle='--', 
                   label=f'Media: {df_fare_analysis["Fare_Log10"].mean():.3f}')
axes[0, 1].axvline(df_fare_analysis['Fare_Log10'].median(), color='green', linestyle='-', 
                   label=f'Mediana: {df_fare_analysis["Fare_Log10"].median():.3f}')
axes[0, 1].legend()
axes[0, 1].grid(axis='y', alpha=0.3)

# 3. Box Plot Fare Original
axes[1, 0].boxplot(df_fare_analysis['Fare'], vert=True)
axes[1, 0].set_title('Box Plot - Fare Original', fontsize=14, fontweight='bold')
axes[1, 0].set_ylabel('Tarifa ($)', fontsize=11)
axes[1, 0].grid(axis='y', alpha=0.3)

# 4. Box Plot Fare Logar√≠tmico
axes[1, 1].boxplot(df_fare_analysis['Fare_Log10'], vert=True)
axes[1, 1].set_title('Box Plot - Fare Transformado (Log10)', fontsize=14, fontweight='bold')
axes[1, 1].set_ylabel('Log10(Tarifa)', fontsize=11)
axes[1, 1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.suptitle('Comparaci√≥n: Transformaci√≥n Logar√≠tmica de Fare', 
             y=1.01, fontsize=16, fontweight='bold')
plt.savefig('10_transformacion_logaritmica_fare.png', dpi=300, bbox_inches='tight')
plt.show()


# --- An√°lisis de Age (opcional) ---
print("\n" + "="*60)
print("AN√ÅLISIS: ¬øAGE NECESITA TRANSFORMACI√ìN LOGAR√çTMICA?")
print("="*60)

df_age_analysis = df[df['Age'].notna()].copy()
print(f"\nRegistros con Age v√°lida: {len(df_age_analysis)}")

# Estad√≠sticas de Age
print("\nüìä Estad√≠sticas de 'Age' (Original):")
print(f"  M√≠nimo:  {df_age_analysis['Age'].min():.2f} a√±os")
print(f"  Q1:      {df_age_analysis['Age'].quantile(0.25):.2f} a√±os")
print(f"  Mediana: {df_age_analysis['Age'].median():.2f} a√±os")
print(f"  Q3:      {df_age_analysis['Age'].quantile(0.75):.2f} a√±os")
print(f"  M√°ximo:  {df_age_analysis['Age'].max():.2f} a√±os")
print(f"  Media:   {df_age_analysis['Age'].mean():.2f} a√±os")

skewness_age = df_age_analysis['Age'].skew()
print(f"\n  Asimetr√≠a (Skewness): {skewness_age:.3f}")
if abs(skewness_age) < 0.5:
    print("    ‚Üí Skewness < 0.5: Distribuci√≥n relativamente sim√©trica")
    print("    ‚Üí NO NECESITA transformaci√≥n logar√≠tmica")
else:
    print("    ‚Üí Skewness >= 0.5: Distribuci√≥n moderadamente sesgada")
    print("    ‚Üí Transformaci√≥n logar√≠tmica podr√≠a ser beneficiosa")


# --- Comparaci√≥n con correlaciones ---
print("\n" + "="*60)
print("IMPACTO EN CORRELACIONES")
print("="*60)

# Crear DataFrame con Fare transformado
df_compare = df[['Age', 'Fare', 'Pclass', 'Survived']].dropna()
df_compare_log = df_compare.copy()
df_compare_log['Fare_Log10'] = np.log10(df_compare_log['Fare'].replace(0, 0.01))  # Evitar log(0)

print("\n--- Correlaci√≥n con 'Survived' ---")
print(f"  Fare (Original):     {df_compare['Fare'].corr(df_compare['Survived']):.4f}")
print(f"  Fare_Log10:          {df_compare_log['Fare_Log10'].corr(df_compare_log['Survived']):.4f}")
print(f"\n--- Correlaci√≥n con 'Pclass' ---")
print(f"  Fare (Original):     {df_compare['Fare'].corr(df_compare['Pclass']):.4f}")
print(f"  Fare_Log10:          {df_compare_log['Fare_Log10'].corr(df_compare_log['Pclass']):.4f}")


# ============================================
# GUARDAR DATASETS TRANSFORMADOS EN CSV
# ============================================

print("\n" + "="*80)
print("CREAR DATASET √öNICO CON TODAS LAS TRANSFORMACIONES")
print("="*80)

# Crear dataset combinado basado en One-Hot Encoding
df_transformado = df_one_hot.copy()

# Agregar FamilySize e IsAlone (Feature Engineering)
print("\n‚úì A√±adiendo caracter√≠sticas de Feature Engineering...")
df_transformado['FamilySize'] = df_transformado['SibSp'] + df_transformado['Parch'] + 1
df_transformado['IsAlone'] = (df_transformado['FamilySize'] == 1).astype(int)

# Agregar transformaci√≥n logar√≠tmica de Fare
print("‚úì A√±adiendo transformaci√≥n logar√≠tmica de Fare")
df_transformado['Fare_Log10'] = np.log10(df_transformado['Fare'].replace(0, 0.01))

# Mostrar informaci√≥n del dataset combinado
print("\nüìä INFORMACI√ìN DEL DATASET TRANSFORMADO:")
print(f"  Total de filas: {len(df_transformado)}")
print(f"  Total de columnas: {len(df_transformado.columns)}")
print(f"\n  Columnas incluidas:")

columnas_originales = ['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']
columnas_one_hot = ['Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']
columnas_feature_eng = ['FamilySize', 'IsAlone']
columnas_transform = ['Fare_Log10']

print(f"\n  ‚îú‚îÄ Originales: {columnas_originales}")
print(f"  ‚îú‚îÄ One-Hot Encoding: {columnas_one_hot}")
print(f"  ‚îú‚îÄ Feature Engineering: {columnas_feature_eng}")
print(f"  ‚îî‚îÄ Transformaci√≥n Logar√≠tmica: {columnas_transform}")

# Guardar el dataset combinado
print("\n" + "="*80)
print("GUARDANDO DATASET TRANSFORMADO")
print("="*80)

archivo_salida = 'Titanic_Transformado.csv'
df_transformado.to_csv(archivo_salida, index=False, encoding='utf-8')

print(f"\n‚úì Archivo guardado exitosamente: {archivo_salida}")
print(f"  Ubicaci√≥n: ./projects/{archivo_salida}")
print(f"  Filas: {len(df_transformado)}")
print(f"  Columnas: {len(df_transformado.columns)}")


# ============================================
# CONCLUSIONES DEL AN√ÅLISIS EXPLORATORIO
# ============================================

"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         CONCLUSIONES PRINCIPALES - AN√ÅLISIS EXPLORATORIO TITANIC           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
üìå 1. LA CLASE SOCIAL FUE EL FACTOR DETERMINANTE
   ‚îú‚îÄ Pasajeros de 1¬™ y 2¬™ clase tuvieron mayor tasa de supervivencia
   ‚îú‚îÄ La clase determina acceso f√≠sico a botes salvavidas (ubicaci√≥n del camarote)
   ‚îú‚îÄ Relaci√≥n inversa fuerte: Pclass vs Survived (correlaci√≥n: -0.34)
   ‚îî‚îÄ Conclusi√≥n: El Titanic era un reflejo de la desigualdad social de la √©poca


 2. EL DINERO IMPORTABA M√ÅS QUE SE PENSABA
   ‚îú‚îÄ Tarifa pagada fuertemente correlacionada con supervivencia (r ‚âà 0.26)
   ‚îú‚îÄ Tarifas var√≠an desde $4 a $512 (diferencia de 128x)
   ‚îú‚îÄ Media de tarifa ($32) >> Mediana ($14) ‚Üí Fuerte concentraci√≥n de riqueza
   ‚îú‚îÄ 50% de pasajeros pag√≥ menos de $14.45
   ‚îú‚îÄ Distribuci√≥n de tarifa es sesgada ‚Üí Requiere transformaci√≥n logar√≠tmica
   ‚îî‚îÄ Conclusi√≥n: La riqueza fue un predictor crucial de supervivencia


 3. EDAD: FACTOR MENOS IMPORTANTE DE LO ESPERADO
   ‚îú‚îÄ Edad media: 29.7 a√±os, Mediana: 28 a√±os
   ‚îú‚îÄ Distribuci√≥n relativamente sim√©trica (skewness bajo)
   ‚îú‚îÄ Sin correlaci√≥n fuerte con clase social o tarifa
   ‚îú‚îÄ Presencia de pasajeros desde beb√©s (0 a√±os) hasta ancianos (80 a√±os)
   ‚îî‚îÄ Conclusi√≥n: La edad afect√≥ pero no fue determinante respecto a clase


 4. ESTRUCTURA FAMILIAR Y VIAJE SOLITARIO
   ‚îú‚îÄ Promedio de hermanos/c√≥nyuges: 0.52 ‚Üí Mayor√≠a viajaba solo
   ‚îú‚îÄ Promedio de padres/hijos: 0.38 ‚Üí Pocas familias numerosas
   ‚îú‚îÄ Correlaci√≥n entre SibSp y Parch: 0.415 ‚Üí Complementarias
   ‚îú‚îÄ Feature Engineering cre√≥: FamilySize, IsAlone
   ‚îú‚îÄ Viajar con familia aument√≥ probabilidades de supervivencia
   ‚îî‚îÄ Conclusi√≥n: La familia brind√≥ apoyo emocional y pr√°ctico durante evacuaci√≥n


 5. DISTRIBUCIONES Y OUTLIERS
   ‚îú‚îÄ Age: Distribuci√≥n normal, sin necesidad de transformaci√≥n
   ‚îú‚îÄ Fare: Altamente sesgada (skewness > 1), necesita log10
   ‚îú‚îÄ Edad outliers: ~10 personas mayores de 65 a√±os
   ‚îú‚îÄ Fare outliers: ~50-100 personas con tarifas > $200
   ‚îú‚îÄ Dataset limpio reduce dimensi√≥n ~10-15% eliminando extremos
   ‚îî‚îÄ Conclusi√≥n: Transformaci√≥n logar√≠tmica mejora modelos predictivos




 7. DESBALANCE DE G√âNERO
   ‚îú‚îÄ Mujeres: ~314 (35%)
   ‚îú‚îÄ Hombres: ~577 (65%)
   ‚îú‚îÄ Proporci√≥n hombre:mujer ‚âà 2:1
   ‚îú‚îÄ Protocolo "Mujeres y ni√±os primero" fue parcialmente aplicado
   ‚îî‚îÄ Conclusi√≥n: El g√©nero influy√≥ en oportunidades de supervivencia


  8. PUERTOS DE EMBARQUE
   ‚îú‚îÄ Southampton (S): ~644 pasajeros (72%)
   ‚îú‚îÄ Cherbourg (C): ~168 pasajeros (19%)
   ‚îú‚îÄ Queenstown (Q): ~77 pasajeros (9%)
   ‚îú‚îÄ Distribuci√≥n reflejaba rutas comerciales inglesas
   ‚îî‚îÄ Conclusi√≥n: Mayor√≠a embarc√≥ en Reino Unido








‚ïë  INSIGHT FINAL: El Titanic no fue un desastre aleatorio, sino un reflejo   ‚ïë
‚ïë  de la sociedad de 1912. Los datos revelan que la supervivencia dependi√≥    ‚ïë
‚ïë  en primer lugar de factores socioecon√≥micos (clase y dinero), mucho m√°s    ‚ïë
‚ïë  que de azar o caracter√≠sticas personales como edad o g√©nero.              ‚ïë
‚ïë  Este an√°lisis demuestra el poder de Data Science para extraer verdades     ‚ïë
‚ïë  hist√≥ricas de n√∫meros crudos.                                             ‚ïë



"""


